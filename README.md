# ğŸ§  AI-Powered Code & Developer Assistant

A local GenAI-powered coding agent that helps you **analyze, debug, and optimize Python code** â€” with or without LLMs. Designed for developers, learners, and security-conscious teams, it uses `mistral-7b-openorca.Q4_K_M.gguf` to run completely offline.

---

## ğŸ¯ Project Objective
To create a dual-mode code assistant that:
- Understands and explains code behavior line-by-line
- Automatically detects and fixes bugs in code
- Refactors and optimizes scripts for readability and performance
- Engages in contextual multi-turn conversations (Agent Mode)

---

## ğŸš€ Key Features
- **Explain Mode** â€“ Understand any Python code snippet
- **Fix Mode** â€“ Auto-debug common issues and broken logic
- **Optimize Mode** â€“ Refactor for readability, performance, or PEP8
- **Agent Chat Mode** â€“ LLM-powered, memory-aware conversations
- **Markdown Export** â€“ Save results for learning or review
- **Local Execution** â€“ No cloud, no OpenAI API â€” 100% offline

---

## ğŸ§  Conceptual Study
Want to dive deeper into how this works?
ğŸ‘‰ [Read the Full Conceptual Study â†’](https://github.com/Pre123140/AI_CODE_DEVELOPER_ASSISTANT/blob/main/AI_CODE_DEVELOPER_ASSISTANT.pdf)

---

## ğŸ› ï¸ Tech Stack
- `mistral-7b-openorca.Q4_K_M.gguf` via GPT4All
- `LangChain` with memory support
- `Streamlit` UI (dual apps)
- Python core: `ast`, `tempfile`, `markdown`, etc.


---

## ğŸ“ Folder Structure
```
AI_Code_Developer_Assistant/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b-openorca.Q4_K_M.gguf
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ example_buggy_code.py
â”‚   â”œâ”€â”€ example_insecure_code.py
â”‚   â””â”€â”€ example_unoptimized_code.py
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ *.csv, *.txt, *.py (auto-generated reports)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                      # Rule-based version
â”‚   â”œâ”€â”€ ai_code_assistant_app.py   # LLM-powered assistant
â”‚   â”œâ”€â”€ code_analysis.py
â”‚   â”œâ”€â”€ code_optimizer.py
â”‚   â”œâ”€â”€ auto_debugger.py
â”‚   â”œâ”€â”€ llm_agent.py
â”‚   â””â”€â”€ prompt_templates.py
```

---

## âš™ï¸ How to Run the Project
## âš™ï¸ How to Run the Project

### 1. ğŸ“‚ Clone the Repository
```bash
git clone https://github.com/yourname/AI_Code_Developer_Assistant
cd AI_Code_Developer_Assistant
```

### 2. âœ¨ Set Up Environment
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 3. âš–ï¸ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. ğŸ¤– Add Your LLM Model
Place `mistral-7b-openorca.Q4_K_M.gguf` in the `/models/` folder.

### 5. ğŸ”¹ Launch the App
```bash
streamlit run src/app.py                    # Static mode (no LLM)
streamlit run src/ai_code_assistant_app.py  # Agentic mode (LLM)
```


---



## âœ¨ Project Highlights
- **Local-First AI**: No internet required â€” privacy-preserving
- **Dual Mode**: Toggle between rule-based and LLM-powered analysis
- **Multi-Agent Ready**: Modular structure for extending capabilities
- **Developer-Friendly**: Markdown outputs, no API costs

---

## ğŸ“œ License

This project is open for educational use only. For commercial deployment, contact the author.

---

## ğŸ“¬ Contact
If you'd like to learn more or collaborate on projects or other initiatives, feel free to connect on [LinkedIn](https://www.linkedin.com/in/prerna-burande-99678a1bb/) or check out my [portfolio site](https://youtheleader.com/).

